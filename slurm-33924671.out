12/04/2023 14:50:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
12/04/2023 14:50:40 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=False, do_train=True, eval_all_checkpoints=False, eval_data_file=None, eval_longer=False, eval_prefix='data/test-experiment//nv_f/testexp', evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=0.001, line_by_line=True, local_rank=-1, logging_steps=1, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, n_tokens=1, no_cuda=False, nonce_mlm_probability=None, num_train_epochs=70.0, output_dir='checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=16, per_gpu_train_batch_size=4, pll_whole_sentence=False, prepend_space=None, replace_by_mask_probability=0.8, replace_by_random_probability=0.1, save_embeddings_for_stimuli_splits=['dev'], save_steps=-1, save_total_limit=None, seed=1, server_ip='', server_port='', should_continue=False, token_ids=[2, 3], token_sequences=[[2], [3]], token_sequences_text=['[unused1]', '[unused2]'], tokenizer_name=None, tokens=['[unused1]', '[unused2]'], train_data_file='data/test-experiment//nv_f/testexp_finetune.txt', unused_list=['[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]', '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]', '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]', '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]', '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]', '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]', '[unused100]', '[unused101]', '[unused102]', '[unused103]', '[unused104]', '[unused105]', '[unused106]', '[unused107]', '[unused108]', '[unused109]', '[unused110]', '[unused111]', '[unused112]', '[unused113]', '[unused114]', '[unused115]', '[unused116]', '[unused117]', '[unused118]', '[unused119]', '[unused120]', '[unused121]', '[unused122]', '[unused123]', '[unused124]', '[unused125]', '[unused126]', '[unused127]', '[unused128]', '[unused129]', '[unused130]', '[unused131]', '[unused132]', '[unused133]', '[unused134]', '[unused135]', '[unused136]', '[unused137]', '[unused138]', '[unused139]', '[unused140]', '[unused141]', '[unused142]', '[unused143]', '[unused144]', '[unused145]', '[unused146]', '[unused147]', '[unused148]', '[unused149]', '[unused150]', '[unused151]', '[unused152]', '[unused153]', '[unused154]', '[unused155]', '[unused156]', '[unused157]', '[unused158]', '[unused159]', '[unused160]', '[unused161]', '[unused162]', '[unused163]', '[unused164]', '[unused165]', '[unused166]', '[unused167]', '[unused168]', '[unused169]', '[unused170]', '[unused171]', '[unused172]', '[unused173]', '[unused174]', '[unused175]', '[unused176]', '[unused177]', '[unused178]', '[unused179]', '[unused180]', '[unused181]', '[unused182]', '[unused183]', '[unused184]', '[unused185]', '[unused186]', '[unused187]', '[unused188]', '[unused189]', '[unused190]', '[unused191]', '[unused192]', '[unused193]', '[unused194]', '[unused195]', '[unused196]', '[unused197]', '[unused198]', '[unused199]', '[unused200]', '[unused201]', '[unused202]', '[unused203]', '[unused204]', '[unused205]', '[unused206]', '[unused207]', '[unused208]', '[unused209]', '[unused210]', '[unused211]', '[unused212]', '[unused213]', '[unused214]', '[unused215]', '[unused216]', '[unused217]', '[unused218]', '[unused219]', '[unused220]', '[unused221]', '[unused222]', '[unused223]', '[unused224]', '[unused225]', '[unused226]', '[unused227]', '[unused228]', '[unused229]', '[unused230]', '[unused231]', '[unused232]', '[unused233]', '[unused234]', '[unused235]', '[unused236]', '[unused237]', '[unused238]', '[unused239]', '[unused240]', '[unused241]', '[unused242]', '[unused243]', '[unused244]', '[unused245]', '[unused246]', '[unused247]', '[unused248]', '[unused249]', '[unused250]', '[unused251]', '[unused252]', '[unused253]', '[unused254]', '[unused255]', '[unused256]', '[unused257]', '[unused258]', '[unused259]', '[unused260]', '[unused261]', '[unused262]', '[unused263]', '[unused264]', '[unused265]', '[unused266]', '[unused267]', '[unused268]', '[unused269]', '[unused270]', '[unused271]', '[unused272]', '[unused273]', '[unused274]', '[unused275]', '[unused276]', '[unused277]', '[unused278]', '[unused279]', '[unused280]', '[unused281]', '[unused282]', '[unused283]', '[unused284]', '[unused285]', '[unused286]', '[unused287]', '[unused288]', '[unused289]', '[unused290]', '[unused291]', '[unused292]', '[unused293]', '[unused294]', '[unused295]', '[unused296]', '[unused297]', '[unused298]', '[unused299]', '[unused300]', '[unused301]', '[unused302]', '[unused303]', '[unused304]', '[unused305]', '[unused306]', '[unused307]', '[unused308]', '[unused309]', '[unused310]', '[unused311]', '[unused312]', '[unused313]', '[unused314]', '[unused315]', '[unused316]', '[unused317]', '[unused318]', '[unused319]', '[unused320]', '[unused321]', '[unused322]', '[unused323]', '[unused324]', '[unused325]', '[unused326]', '[unused327]', '[unused328]', '[unused329]', '[unused330]', '[unused331]', '[unused332]', '[unused333]', '[unused334]', '[unused335]', '[unused336]', '[unused337]', '[unused338]', '[unused339]', '[unused340]', '[unused341]', '[unused342]', '[unused343]', '[unused344]', '[unused345]', '[unused346]', '[unused347]', '[unused348]', '[unused349]', '[unused350]', '[unused351]', '[unused352]', '[unused353]', '[unused354]', '[unused355]', '[unused356]', '[unused357]', '[unused358]', '[unused359]', '[unused360]', '[unused361]', '[unused362]', '[unused363]', '[unused364]', '[unused365]', '[unused366]', '[unused367]', '[unused368]', '[unused369]', '[unused370]', '[unused371]', '[unused372]', '[unused373]', '[unused374]', '[unused375]', '[unused376]', '[unused377]', '[unused378]', '[unused379]', '[unused380]', '[unused381]', '[unused382]', '[unused383]', '[unused384]', '[unused385]', '[unused386]', '[unused387]', '[unused388]', '[unused389]', '[unused390]', '[unused391]', '[unused392]', '[unused393]', '[unused394]', '[unused395]', '[unused396]', '[unused397]', '[unused398]', '[unused399]', '[unused400]', '[unused401]', '[unused402]', '[unused403]', '[unused404]', '[unused405]', '[unused406]', '[unused407]', '[unused408]', '[unused409]', '[unused410]', '[unused411]', '[unused412]', '[unused413]', '[unused414]', '[unused415]', '[unused416]', '[unused417]', '[unused418]', '[unused419]', '[unused420]', '[unused421]', '[unused422]', '[unused423]', '[unused424]', '[unused425]', '[unused426]', '[unused427]', '[unused428]', '[unused429]', '[unused430]', '[unused431]', '[unused432]', '[unused433]', '[unused434]', '[unused435]', '[unused436]', '[unused437]', '[unused438]', '[unused439]', '[unused440]', '[unused441]', '[unused442]', '[unused443]', '[unused444]', '[unused445]', '[unused446]', '[unused447]', '[unused448]', '[unused449]', '[unused450]', '[unused451]', '[unused452]', '[unused453]', '[unused454]', '[unused455]', '[unused456]', '[unused457]', '[unused458]', '[unused459]', '[unused460]', '[unused461]', '[unused462]', '[unused463]', '[unused464]', '[unused465]', '[unused466]', '[unused467]', '[unused468]', '[unused469]', '[unused470]', '[unused471]', '[unused472]', '[unused473]', '[unused474]', '[unused475]', '[unused476]', '[unused477]', '[unused478]', '[unused479]', '[unused480]', '[unused481]', '[unused482]', '[unused483]', '[unused484]', '[unused485]', '[unused486]', '[unused487]', '[unused488]', '[unused489]', '[unused490]', '[unused491]', '[unused492]', '[unused493]', '[unused494]', '[unused495]', '[unused496]', '[unused497]', '[unused498]', '[unused499]', '[unused500]', '[unused501]', '[unused502]', '[unused503]', '[unused504]', '[unused505]', '[unused506]', '[unused507]', '[unused508]', '[unused509]', '[unused510]', '[unused511]', '[unused512]', '[unused513]', '[unused514]', '[unused515]', '[unused516]', '[unused517]', '[unused518]', '[unused519]', '[unused520]', '[unused521]', '[unused522]', '[unused523]', '[unused524]', '[unused525]', '[unused526]', '[unused527]', '[unused528]', '[unused529]', '[unused530]', '[unused531]', '[unused532]', '[unused533]', '[unused534]', '[unused535]', '[unused536]', '[unused537]', '[unused538]', '[unused539]', '[unused540]', '[unused541]', '[unused542]', '[unused543]', '[unused544]', '[unused545]', '[unused546]', '[unused547]', '[unused548]', '[unused549]', '[unused550]', '[unused551]', '[unused552]', '[unused553]', '[unused554]', '[unused555]', '[unused556]', '[unused557]', '[unused558]', '[unused559]', '[unused560]', '[unused561]', '[unused562]', '[unused563]', '[unused564]', '[unused565]', '[unused566]', '[unused567]', '[unused568]', '[unused569]', '[unused570]', '[unused571]', '[unused572]', '[unused573]', '[unused574]', '[unused575]', '[unused576]', '[unused577]', '[unused578]', '[unused579]', '[unused580]', '[unused581]', '[unused582]', '[unused583]', '[unused584]', '[unused585]', '[unused586]', '[unused587]', '[unused588]', '[unused589]', '[unused590]', '[unused591]', '[unused592]', '[unused593]', '[unused594]', '[unused595]', '[unused596]', '[unused597]', '[unused598]', '[unused599]', '[unused600]', '[unused601]', '[unused602]', '[unused603]', '[unused604]', '[unused605]', '[unused606]', '[unused607]', '[unused608]', '[unused609]', '[unused610]', '[unused611]', '[unused612]', '[unused613]', '[unused614]', '[unused615]', '[unused616]', '[unused617]', '[unused618]', '[unused619]', '[unused620]', '[unused621]', '[unused622]', '[unused623]', '[unused624]', '[unused625]', '[unused626]', '[unused627]', '[unused628]', '[unused629]', '[unused630]', '[unused631]', '[unused632]', '[unused633]', '[unused634]', '[unused635]', '[unused636]', '[unused637]', '[unused638]', '[unused639]', '[unused640]', '[unused641]', '[unused642]', '[unused643]', '[unused644]', '[unused645]', '[unused646]', '[unused647]', '[unused648]', '[unused649]', '[unused650]', '[unused651]', '[unused652]', '[unused653]', '[unused654]', '[unused655]', '[unused656]', '[unused657]', '[unused658]', '[unused659]', '[unused660]', '[unused661]', '[unused662]', '[unused663]', '[unused664]', '[unused665]', '[unused666]', '[unused667]', '[unused668]', '[unused669]', '[unused670]', '[unused671]', '[unused672]', '[unused673]', '[unused674]', '[unused675]', '[unused676]', '[unused677]', '[unused678]', '[unused679]', '[unused680]', '[unused681]', '[unused682]', '[unused683]', '[unused684]', '[unused685]', '[unused686]', '[unused687]', '[unused688]', '[unused689]', '[unused690]', '[unused691]', '[unused692]', '[unused693]', '[unused694]', '[unused695]', '[unused696]', '[unused697]', '[unused698]', '[unused699]', '[unused700]', '[unused701]', '[unused702]', '[unused703]', '[unused704]', '[unused705]', '[unused706]', '[unused707]', '[unused708]', '[unused709]', '[unused710]', '[unused711]', '[unused712]', '[unused713]', '[unused714]', '[unused715]', '[unused716]', '[unused717]', '[unused718]', '[unused719]', '[unused720]', '[unused721]', '[unused722]', '[unused723]', '[unused724]', '[unused725]', '[unused726]', '[unused727]', '[unused728]', '[unused729]', '[unused730]', '[unused731]', '[unused732]', '[unused733]', '[unused734]', '[unused735]', '[unused736]', '[unused737]', '[unused738]', '[unused739]', '[unused740]', '[unused741]', '[unused742]', '[unused743]', '[unused744]', '[unused745]', '[unused746]', '[unused747]', '[unused748]', '[unused749]', '[unused750]', '[unused751]', '[unused752]', '[unused753]', '[unused754]', '[unused755]', '[unused756]', '[unused757]', '[unused758]', '[unused759]', '[unused760]', '[unused761]', '[unused762]', '[unused763]', '[unused764]', '[unused765]', '[unused766]', '[unused767]', '[unused768]', '[unused769]', '[unused770]', '[unused771]', '[unused772]', '[unused773]', '[unused774]', '[unused775]', '[unused776]', '[unused777]', '[unused778]', '[unused779]', '[unused780]', '[unused781]', '[unused782]', '[unused783]', '[unused784]', '[unused785]', '[unused786]', '[unused787]', '[unused788]', '[unused789]', '[unused790]', '[unused791]', '[unused792]', '[unused793]', '[unused794]', '[unused795]', '[unused796]', '[unused797]', '[unused798]', '[unused799]', '[unused800]', '[unused801]', '[unused802]', '[unused803]', '[unused804]', '[unused805]', '[unused806]', '[unused807]', '[unused808]', '[unused809]', '[unused810]', '[unused811]', '[unused812]', '[unused813]', '[unused814]', '[unused815]', '[unused816]', '[unused817]', '[unused818]', '[unused819]', '[unused820]', '[unused821]', '[unused822]', '[unused823]', '[unused824]', '[unused825]', '[unused826]', '[unused827]', '[unused828]', '[unused829]', '[unused830]', '[unused831]', '[unused832]', '[unused833]', '[unused834]', '[unused835]', '[unused836]', '[unused837]', '[unused838]', '[unused839]', '[unused840]', '[unused841]', '[unused842]', '[unused843]', '[unused844]', '[unused845]', '[unused846]', '[unused847]', '[unused848]', '[unused849]', '[unused850]', '[unused851]', '[unused852]', '[unused853]', '[unused854]', '[unused855]', '[unused856]', '[unused857]', '[unused858]', '[unused859]', '[unused860]', '[unused861]', '[unused862]', '[unused863]', '[unused864]', '[unused865]', '[unused866]', '[unused867]', '[unused868]', '[unused869]', '[unused870]', '[unused871]', '[unused872]', '[unused873]', '[unused874]', '[unused875]', '[unused876]', '[unused877]', '[unused878]', '[unused879]', '[unused880]', '[unused881]', '[unused882]', '[unused883]', '[unused884]', '[unused885]', '[unused886]', '[unused887]', '[unused888]', '[unused889]', '[unused890]', '[unused891]', '[unused892]', '[unused893]', '[unused894]', '[unused895]', '[unused896]', '[unused897]', '[unused898]', '[unused899]', '[unused900]', '[unused901]', '[unused902]', '[unused903]', '[unused904]', '[unused905]', '[unused906]', '[unused907]', '[unused908]', '[unused909]', '[unused910]', '[unused911]', '[unused912]', '[unused913]', '[unused914]', '[unused915]', '[unused916]', '[unused917]', '[unused918]', '[unused919]', '[unused920]', '[unused921]', '[unused922]', '[unused923]', '[unused924]', '[unused925]', '[unused926]', '[unused927]', '[unused928]', '[unused929]', '[unused930]', '[unused931]', '[unused932]', '[unused933]', '[unused934]', '[unused935]', '[unused936]', '[unused937]', '[unused938]', '[unused939]', '[unused940]', '[unused941]', '[unused942]', '[unused943]', '[unused944]', '[unused945]', '[unused946]', '[unused947]', '[unused948]', '[unused949]', '[unused950]', '[unused951]', '[unused952]', '[unused953]', '[unused954]', '[unused955]', '[unused956]', '[unused957]', '[unused958]', '[unused959]', '[unused960]', '[unused961]', '[unused962]', '[unused963]', '[unused964]', '[unused965]', '[unused966]', '[unused967]', '[unused968]', '[unused969]', '[unused970]', '[unused971]', '[unused972]', '[unused973]', '[unused974]', '[unused975]', '[unused976]', '[unused977]', '[unused978]', '[unused979]', '[unused980]', '[unused981]', '[unused982]', '[unused983]', '[unused984]', '[unused985]', '[unused986]', '[unused987]', '[unused988]', '[unused989]', '[unused990]', '[unused991]', '[unused992]', '[unused993]'], unused_list_ids=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998], unused_list_ids_set={1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998}, warmup_steps=0, weight_decay=0.0)
12/04/2023 14:50:40 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:50:40 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:40 - INFO - __main__ -     Batch size = 16
Vocab size in initial config: 30522
Vocab size in final tokenizer: 30522
tokens: [unused1] [unused2]
token ids: 2 3
token sequences: [[2], [3]]
token sequences text: ['[unused1]', '[unused2]']
Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]Evaluating: 100%|ââââââââââ| 1/1 [00:03<00:00,  3.36s/it]Evaluating: 100%|ââââââââââ| 1/1 [00:03<00:00,  3.36s/it]
12/04/2023 14:50:44 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:50:44 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:44 - INFO - __main__ -     Batch size = 16
Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]Evaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.46it/s]
12/04/2023 14:50:44 - INFO - __main__ -   epoch: -1
12/04/2023 14:50:44 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:50:44 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:50:44 - INFO - __main__ -   ctg1_diff: 0.0
/home/soahmed/anaconda3/envs/cxg/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
12/04/2023 14:50:44 - INFO - __main__ -   ***** Running training *****
12/04/2023 14:50:44 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:44 - INFO - __main__ -     Num Epochs = 70
12/04/2023 14:50:44 - INFO - __main__ -     Instantaneous batch size per GPU = 4
12/04/2023 14:50:44 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4
12/04/2023 14:50:44 - INFO - __main__ -     Gradient Accumulation steps = 1
12/04/2023 14:50:44 - INFO - __main__ -     Total optimization steps = 70
Not freezing embeddings weight parameter bert.embeddings.word_embeddings.weight of size torch.Size([30522, 768])
TRAIN DATASET: [('How nice, how awfully nice of [unused1] to call.', '[unused1]'), ('How nice, how awfully nice that [unused2] called.', '[unused2]')]
Epoch:   0%|          | 0/70 [00:00<?, ?it/s]12/04/2023 14:50:44 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:50:44 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:44 - INFO - __main__ -     Batch size = 16

Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.83it/s]
12/04/2023 14:50:44 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:50:44 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:44 - INFO - __main__ -     Batch size = 16

Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.27it/s]
12/04/2023 14:50:44 - INFO - __main__ -   epoch: -1
12/04/2023 14:50:44 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:50:44 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:50:44 - INFO - __main__ -   ctg1_diff: 0.0
12/04/2023 14:50:46 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint--1
TRAIN STIMULI: [('How nice, how awfully nice of [unused1] to call.', '[unused1]'), ('How nice, how awfully nice that [unused2] called.', '[unused2]')]
STIMULI NUMBERS:
 STIMULI NUMBER: 2
_dev_cat0 STIMULI NUMBER: 2
_dev_cat1 STIMULI NUMBER: 2
TOKEN IDS TENSOR: tensor([2, 3], device='cuda:0')
TOKENIZED: {'input_ids': tensor([ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
         102,    0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])}

Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:50:46 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:50:46 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:46 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,    2, 2000, 2655,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129, 9643,  103, 3835, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, 2135, -100, -100, -100, -100, 1012,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 0.011970981024205685


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.99it/s]
12/04/2023 14:50:46 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:50:46 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:46 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.67it/s]
12/04/2023 14:50:46 - INFO - __main__ -   epoch: 0
12/04/2023 14:50:46 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:50:46 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:50:46 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.78it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.78it/s]
12/04/2023 14:50:48 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-0
Epoch:   1%|â         | 1/70 [00:04<04:38,  4.04s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:50:48 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:50:48 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:48 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170,  103,
          102,    0],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,    2, 2000, 2655,
          103,  102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1012,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         1012, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 0.1851542592048645


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.21it/s]
12/04/2023 14:50:48 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:50:48 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:48 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.23it/s]
12/04/2023 14:50:48 - INFO - __main__ -   epoch: 1
12/04/2023 14:50:48 - INFO - __main__ -   all_diff: 0.25
12/04/2023 14:50:48 - INFO - __main__ -   ctg0_diff: 0.5
12/04/2023 14:50:48 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.00it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.99it/s]
12/04/2023 14:50:50 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-1
Epoch:   3%|â         | 2/70 [00:06<03:17,  2.90s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:50:50 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:50:50 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:50 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,    2, 2000, 2655,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129,  103, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, 9643, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 1.6737345457077026


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.14it/s]
12/04/2023 14:50:50 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:50:50 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:50 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.53it/s]
12/04/2023 14:50:50 - INFO - __main__ -   epoch: 2
12/04/2023 14:50:50 - INFO - __main__ -   all_diff: 0.25
12/04/2023 14:50:50 - INFO - __main__ -   ctg0_diff: 0.5
12/04/2023 14:50:50 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.06it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.06it/s]
12/04/2023 14:50:52 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-2
Epoch:   4%|â         | 3/70 [00:08<02:47,  2.50s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:50:52 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:50:52 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:52 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129, 21002,  1010,  2129,  9643,  2135,  3835,  2008,     3,
          2170,  1012,   102,     0],
        [  101,  2129,  3835,  1010,  2129,  9643,  2135,  3835,  1997,     2,
          2000,  2655,  1012,   102]], device='cuda:0')
labels: tensor([[-100, -100, 3835, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 5.182066440582275


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.44it/s]
12/04/2023 14:50:52 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:50:52 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:52 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.64it/s]
12/04/2023 14:50:52 - INFO - __main__ -   epoch: 3
12/04/2023 14:50:52 - INFO - __main__ -   all_diff: 0.25
12/04/2023 14:50:52 - INFO - __main__ -   ctg0_diff: 0.5
12/04/2023 14:50:52 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.13it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.12it/s]
12/04/2023 14:50:54 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-3
Epoch:   6%|â         | 4/70 [00:10<02:31,  2.30s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:50:54 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:50:54 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:54 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129,  103, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0],
        [ 101, 2129, 3835, 1010, 2129,  103, 2135, 3835, 1997,    2,  103, 2655,
         1012,  102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, 9643, -100, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, 9643, -100, -100, -100, -100, 2000, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 2.0338873863220215


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.12it/s]
12/04/2023 14:50:54 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:50:54 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:54 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.51it/s]
12/04/2023 14:50:54 - INFO - __main__ -   epoch: 4
12/04/2023 14:50:54 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:50:54 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:50:54 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.03it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.03it/s]
12/04/2023 14:50:56 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-4
Epoch:   7%|â         | 5/70 [00:12<02:25,  2.24s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:50:57 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:50:57 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:57 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,    2, 2000, 2655,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = nan


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.41it/s]
12/04/2023 14:50:57 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:50:57 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:57 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.46it/s]
12/04/2023 14:50:57 - INFO - __main__ -   epoch: 5
12/04/2023 14:50:57 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:50:57 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:50:57 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.00it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.00it/s]
12/04/2023 14:50:58 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-5
Epoch:   9%|â         | 6/70 [00:14<02:19,  2.18s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:50:59 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:50:59 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:59 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,  103, 2000, 2655,
          103,  102],
        [ 101, 2129,  103, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170,  103,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100,    2, -100, -100,
         1012, -100],
        [-100, -100, 3835, -100, -100, -100, -100, -100, -100, -100, -100, 1012,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 4.921718597412109


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.89it/s]
12/04/2023 14:50:59 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:50:59 - INFO - __main__ -     Num examples = 2
12/04/2023 14:50:59 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.97it/s]
12/04/2023 14:50:59 - INFO - __main__ -   epoch: 6
12/04/2023 14:50:59 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:50:59 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:50:59 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.04it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.04it/s]
12/04/2023 14:51:00 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-6
Epoch:  10%|â         | 7/70 [00:16<02:14,  2.13s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:01 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:01 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:01 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129,  103, 1010,  103, 9643, 2135, 3835, 1997,  103, 2000, 2655,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129,  103, 2135, 3835,  103,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, 3835, -100, 2129, -100, -100, -100, 1997,    2, -100, 2655,
         -100, -100],
        [-100, -100, -100, -100, -100, 9643, -100, -100, 2008, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 3.5858476161956787


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.95it/s]
12/04/2023 14:51:01 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:01 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:01 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.21it/s]
12/04/2023 14:51:01 - INFO - __main__ -   epoch: 7
12/04/2023 14:51:01 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:01 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:01 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.16it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.16it/s]
12/04/2023 14:51:02 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-7
Epoch:  11%|ââ        | 8/70 [00:18<02:10,  2.11s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:03 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:03 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:03 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135,  103, 2008,    3, 2170, 1012,
          102,    0],
        [ 101, 2129, 3835, 1010, 2129, 9643,  103, 3835, 1997,    2, 2000,  103,
          103,  102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, 3835, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, 2135, -100, -100, -100, -100, 2655,
         1012, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 1.99686861038208


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 31.14it/s]
12/04/2023 14:51:03 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:03 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:03 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.76it/s]
12/04/2023 14:51:03 - INFO - __main__ -   epoch: 8
12/04/2023 14:51:03 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:03 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:03 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.96it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.96it/s]
12/04/2023 14:51:04 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-8
Epoch:  13%|ââ        | 9/70 [00:20<02:08,  2.11s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:05 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:05 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:05 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835,  1010,  8904, 26187,  2135,  3835,   103,     3,
          2170,  1012,   102,     0],
        [  101,  2129,  3835,  1010,  2129,  9643,  2135,  3835,   103,     2,
          2000,  2655,  1012,   102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, 2129, 9643, -100, -100, 2008, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, 1997, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 4.077438831329346


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.72it/s]
12/04/2023 14:51:05 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:05 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:05 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.00it/s]
12/04/2023 14:51:05 - INFO - __main__ -   epoch: 9
12/04/2023 14:51:05 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:05 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:05 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.15it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.14it/s]
12/04/2023 14:51:06 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-9
Epoch:  14%|ââ        | 10/70 [00:22<02:05,  2.10s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:07 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:07 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:07 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,   103,  3835,  1010,   103,  9643,  2135,  3835,  1997,     2,
          2000, 26972,  1012,   102],
        [  101,  2129,  3835,  1010,  2129,  9643,  2135,  3835,  2008,     3,
          2170,  1012,   102,     0]], device='cuda:0')
labels: tensor([[-100, 2129, -100, -100, 2129, -100, -100, -100, -100, -100, -100, 2655,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 4.4065423011779785


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.83it/s]
12/04/2023 14:51:07 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:07 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:07 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.69it/s]
12/04/2023 14:51:07 - INFO - __main__ -   epoch: 10
12/04/2023 14:51:07 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:07 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:07 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.11it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.11it/s]
12/04/2023 14:51:08 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-10
Epoch:  16%|ââ        | 11/70 [00:24<02:02,  2.07s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:09 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:09 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:09 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835,  1010,   103,  9643,  2135,  3835,  2008,     3,
         14540,  1012,   102,     0],
        [  101,  2129,  3835,  1010,  2129,  9643,  2135,  3835,  1997,     2,
          2000,  2655,  1012,   102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, 2129, -100, -100, -100, -100, -100, 2170, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 4.782414436340332


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.04it/s]
12/04/2023 14:51:09 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:09 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:09 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.12it/s]
12/04/2023 14:51:09 - INFO - __main__ -   epoch: 11
12/04/2023 14:51:09 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:09 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:09 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.03it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.03it/s]
12/04/2023 14:51:11 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-11
Epoch:  17%|ââ        | 12/70 [00:26<02:02,  2.10s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:11 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:11 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:11 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,  103, 2000,  103,
         1012,  102],
        [ 101, 2129,  103, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100,    2, -100, 2655,
         -100, -100],
        [-100, -100, 3835, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 7.880436420440674


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.81it/s]
12/04/2023 14:51:11 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:11 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:11 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.02it/s]
12/04/2023 14:51:11 - INFO - __main__ -   epoch: 12
12/04/2023 14:51:11 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:11 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:11 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.97it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.97it/s]
12/04/2023 14:51:13 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-12
Epoch:  19%|ââ        | 13/70 [00:28<02:00,  2.12s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:13 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:13 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:13 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129,  103, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0],
        [ 101,  103, 3835, 1010, 2129,  103, 2135, 3835, 1997,    2, 2000, 2655,
         1012,  102]], device='cuda:0')
labels: tensor([[-100, -100, 3835, -100, -100, 9643, -100, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, 2129, -100, -100, -100, 9643, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 1.0228331089019775


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 32.28it/s]
12/04/2023 14:51:13 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:13 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:13 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.99it/s]
12/04/2023 14:51:13 - INFO - __main__ -   epoch: 13
12/04/2023 14:51:13 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:13 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:13 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.04it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.03it/s]
12/04/2023 14:51:15 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-13
Epoch:  20%|ââ        | 14/70 [00:30<01:56,  2.09s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:15 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:15 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:15 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129,  103, 2135, 3835, 1997,    2,  103, 2655,
          103,  102],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, 9643, -100, -100, -100, -100, 2000, -100,
         1012, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 1.2235714197158813


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.81it/s]
12/04/2023 14:51:15 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:15 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:15 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.33it/s]
12/04/2023 14:51:15 - INFO - __main__ -   epoch: 14
12/04/2023 14:51:15 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:15 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:15 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.14it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.14it/s]
12/04/2023 14:51:17 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-14
Epoch:  21%|âââ       | 15/70 [00:32<01:53,  2.07s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:17 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:17 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:17 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129,  103, 2135,  103, 2008,    3, 2170, 1012,
          102,    0],
        [ 101, 2129, 3835, 1010,  103, 9643, 2135, 3835, 1997,    2, 2000, 2655,
          103,  102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, 9643, -100, 3835, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, 2129, -100, -100, -100, -100, -100, -100, -100,
         1012, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 2.2034268379211426


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.06it/s]
12/04/2023 14:51:17 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:17 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:17 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.56it/s]
12/04/2023 14:51:17 - INFO - __main__ -   epoch: 15
12/04/2023 14:51:17 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:17 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:17 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.02it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.02it/s]
12/04/2023 14:51:19 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-15
Epoch:  23%|âââ       | 16/70 [00:35<01:52,  2.08s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:19 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:19 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:19 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,    2, 2000,  103,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129,  103, 2135, 3835,  103,    3,  103, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, 3835, -100, -100, -100, -100, -100, -100, -100, -100, 2655,
         -100, -100],
        [-100, -100, -100, -100, -100, 9643, -100, -100, 2008, -100, 2170, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 4.3987717628479


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.81it/s]
12/04/2023 14:51:19 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:19 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:19 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 32.56it/s]
12/04/2023 14:51:19 - INFO - __main__ -   epoch: 16
12/04/2023 14:51:19 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:19 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:19 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.96it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.96it/s]
12/04/2023 14:51:21 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-16
Epoch:  24%|âââ       | 17/70 [00:37<01:48,  2.05s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:21 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:21 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:21 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835,  1010,  2129,   103,  2135,  3835,  2008,     3,
          2170,  1012,   102,     0],
        [  101,  2129,  3835,  1010,  2129,  9643,  6565,   103,  1997,     2,
           103,  2655, 30230,   102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, 9643, -100, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, 2135, 3835, -100, -100, 2000, -100,
         1012, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 4.614083766937256


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.32it/s]
12/04/2023 14:51:21 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:21 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:21 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.57it/s]
12/04/2023 14:51:21 - INFO - __main__ -   epoch: 17
12/04/2023 14:51:21 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:21 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:21 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.12it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.12it/s]
12/04/2023 14:51:23 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-17
Epoch:  26%|âââ       | 18/70 [00:39<01:46,  2.05s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:23 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:23 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:23 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101,  103, 3835, 1010, 2129, 9643, 2135, 3835, 1997,    2, 2000, 2655,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3,  103, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, 2129, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, 1010, -100, -100, -100, -100, -100, -100, 2170, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 3.8703582286834717


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.75it/s]
12/04/2023 14:51:23 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:23 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:23 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.89it/s]
12/04/2023 14:51:23 - INFO - __main__ -   epoch: 18
12/04/2023 14:51:23 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:23 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:23 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.16it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.16it/s]
12/04/2023 14:51:25 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-18
Epoch:  27%|âââ       | 19/70 [00:41<01:45,  2.06s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:25 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:25 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:25 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,    2, 2000,  103,
         1012,  102],
        [ 101,  103, 3835, 1010, 2129, 9643, 2135,  103,  103,    3,  103, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2655,
         -100, -100],
        [-100, 2129, -100, -100, -100, -100, -100, 3835, 2008, -100, 2170, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 5.202554702758789


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.12it/s]
12/04/2023 14:51:26 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:26 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:26 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 32.63it/s]
12/04/2023 14:51:26 - INFO - __main__ -   epoch: 19
12/04/2023 14:51:26 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:26 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:26 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.97it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.97it/s]
12/04/2023 14:51:27 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-19
Epoch:  29%|âââ       | 20/70 [00:43<01:44,  2.08s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:28 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:28 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:28 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101,  103, 3835, 1010, 2129, 9643, 2135, 3835, 1997,    2,  103, 2655,
         1012,  102],
        [ 101, 2129,  103, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, 2129, -100, -100, -100, -100, -100, -100, -100, -100, 2000, -100,
         -100, -100],
        [-100, -100, 3835, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 0.478091835975647


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.24it/s]
12/04/2023 14:51:28 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:28 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:28 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.06it/s]
12/04/2023 14:51:28 - INFO - __main__ -   epoch: 20
12/04/2023 14:51:28 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:28 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:28 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.06it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.06it/s]
12/04/2023 14:51:29 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-20
Epoch:  30%|âââ       | 21/70 [00:45<01:42,  2.09s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:30 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:30 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:30 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835,  1010,  2129,  9643,  2135,   103,  1997,     2,
          2000,  2655,  1012,   102],
        [  101,  2129,  3835,  1010,  2129,  9643,  2135, 17326,  2008,     3,
          2170,  1012,   102,     0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, 3835, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, 3835, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 5.714949131011963


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 32.46it/s]
12/04/2023 14:51:30 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:30 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:30 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.49it/s]
12/04/2023 14:51:30 - INFO - __main__ -   epoch: 21
12/04/2023 14:51:30 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:30 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:30 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.07it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.07it/s]
12/04/2023 14:51:31 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-21
Epoch:  31%|ââââ      | 22/70 [00:47<01:39,  2.08s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:32 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:32 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:32 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129,  103, 2135, 3835,  103,  103, 2000, 2655,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170,  103,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, 1010, -100, 9643, -100, -100, 1997,    2, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1012,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 3.215137481689453


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.91it/s]
12/04/2023 14:51:32 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:32 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:32 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.59it/s]
12/04/2023 14:51:32 - INFO - __main__ -   epoch: 22
12/04/2023 14:51:32 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:32 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:32 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.16it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.16it/s]
12/04/2023 14:51:33 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-22
Epoch:  33%|ââââ      | 23/70 [00:49<01:37,  2.08s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:34 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:34 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:34 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835,   103,   103,  9643,   103,  3835,  1997,     2,
          2000,  2655,  1012,   102],
        [  101,  2129,  3835,  1010,   103,   103,  2135,  3835,  2008,     3,
         27332,  1012,   102,     0]], device='cuda:0')
labels: tensor([[-100, -100, -100, 1010, 2129, -100, 2135, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, 2129, 9643, -100, -100, -100, -100, 2170, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 2.6115119457244873


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.85it/s]
12/04/2023 14:51:34 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:34 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:34 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.76it/s]
12/04/2023 14:51:34 - INFO - __main__ -   epoch: 23
12/04/2023 14:51:34 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:34 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:34 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.05it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.05it/s]
12/04/2023 14:51:36 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-23
Epoch:  34%|ââââ      | 24/70 [00:51<01:36,  2.10s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:36 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:36 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:36 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835,  1010,  2129,  9643,  2135,  3835,  2008,     3,
          2170,   103,   102,     0],
        [  101,  2129,  3835,  1010,  2129,  9643,  2135, 14859,  1997,     2,
          2000,  2655,  1012,   102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1012,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, 3835, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 3.7278037071228027


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.34it/s]
12/04/2023 14:51:36 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:36 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:36 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.89it/s]
12/04/2023 14:51:36 - INFO - __main__ -   epoch: 24
12/04/2023 14:51:36 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:36 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:36 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.01it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.01it/s]
12/04/2023 14:51:38 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-24
Epoch:  36%|ââââ      | 25/70 [00:53<01:34,  2.11s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:38 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:38 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:38 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129,  103,  103,  103, 9643, 2135, 3835, 1997,  103, 2000, 2655,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, 3835, 1010, 2129, -100, -100, -100, -100,    2, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, 2008, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 6.059297561645508


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.51it/s]
12/04/2023 14:51:38 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:38 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:38 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.38it/s]
12/04/2023 14:51:38 - INFO - __main__ -   epoch: 25
12/04/2023 14:51:38 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:38 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:38 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.12it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.12it/s]
12/04/2023 14:51:40 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-25
Epoch:  37%|ââââ      | 26/70 [00:55<01:32,  2.10s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:40 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:40 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:40 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835,  103, 2129, 9643, 2135, 3835, 2008,  103, 2170, 1012,
          102,    0],
        [ 101, 2129, 3835, 1010, 2129,  103, 2135, 3835, 1997,    2, 2000, 2655,
         1012,  102]], device='cuda:0')
labels: tensor([[-100, -100, -100, 1010, -100, -100, -100, -100, -100,    3, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, 9643, -100, 3835, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 5.1525678634643555


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.49it/s]
12/04/2023 14:51:40 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:40 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:40 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.26it/s]
12/04/2023 14:51:40 - INFO - __main__ -   epoch: 26
12/04/2023 14:51:40 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:40 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:40 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.07it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.06it/s]
12/04/2023 14:51:42 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-26
Epoch:  39%|ââââ      | 27/70 [00:58<01:29,  2.09s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:42 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:42 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:42 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129,  103,  103, 3835, 2008,    3, 2170, 1012,
          102,    0],
        [ 101, 2129, 3835,  103, 2129, 9643, 2135, 3835, 1997,    2, 2000, 2655,
         1012,  102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, 9643, 2135, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, 1010, -100, -100, -100, -100, -100, -100, -100, 2655,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 3.247480630874634


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.27it/s]
12/04/2023 14:51:42 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:42 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:42 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.03it/s]
12/04/2023 14:51:42 - INFO - __main__ -   epoch: 27
12/04/2023 14:51:42 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:42 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:42 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.01it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.01it/s]
12/04/2023 14:51:44 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-27
Epoch:  40%|ââââ      | 28/70 [01:00<01:27,  2.08s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:44 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:44 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:44 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010,  103, 9643, 2135, 3835,  103,    2, 2000, 2655,
         1012,  102],
        [ 101,  103,  103,  103, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, 2129, -100, -100, -100, 1997, -100, -100, -100,
         -100, -100],
        [-100, 2129, 3835, 1010, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 3.872286319732666


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.46it/s]
12/04/2023 14:51:44 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:44 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:44 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.12it/s]
12/04/2023 14:51:44 - INFO - __main__ -   epoch: 28
12/04/2023 14:51:44 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:44 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:44 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.09it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.08it/s]
12/04/2023 14:51:46 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-28
Epoch:  41%|âââââ     | 29/70 [01:02<01:24,  2.05s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:46 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:46 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:46 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,   103,  1010,  2129,  9643,  2135,  3835,  1997,     2,
          2000,  2655,  1012,   102],
        [  101,  2129,  3835,  1010,  2129, 23671,  2135,  3835,  2008,     3,
          2170,  1012,   102,     0]], device='cuda:0')
labels: tensor([[-100, -100, 3835, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, 9643, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 5.14746618270874


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.60it/s]
12/04/2023 14:51:46 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:46 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:46 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.92it/s]
12/04/2023 14:51:46 - INFO - __main__ -   epoch: 29
12/04/2023 14:51:46 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:46 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:46 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.95it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.95it/s]
12/04/2023 14:51:48 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-29
Epoch:  43%|âââââ     | 30/70 [01:04<01:22,  2.06s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:48 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:48 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:48 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,  103, 2000, 2655,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100,    2, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 10.994068145751953


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.29it/s]
12/04/2023 14:51:48 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:48 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:48 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.07it/s]
12/04/2023 14:51:48 - INFO - __main__ -   epoch: 30
12/04/2023 14:51:48 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:48 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:48 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.02it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.02it/s]
12/04/2023 14:51:50 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-30
Epoch:  44%|âââââ     | 31/70 [01:06<01:20,  2.06s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:50 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:50 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:50 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835,  1010,  2129, 10742, 28429,  3835,  1997,     2,
          2000,  2655,  1012,   102],
        [  101,  2129,  3835,  1010,  2129,  9643,   103,  3835,  2008,     3,
          2170,  1012,   102,     0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, 9643, 2135, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, 2135, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 3.677769422531128


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.67it/s]
12/04/2023 14:51:51 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:51 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:51 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.21it/s]
12/04/2023 14:51:51 - INFO - __main__ -   epoch: 31
12/04/2023 14:51:51 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:51 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:51 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.00it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.99it/s]
12/04/2023 14:51:52 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-31
Epoch:  46%|âââââ     | 32/70 [01:08<01:18,  2.08s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:53 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:53 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:53 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835, 13000,  2129,  9643,   103,  3835,   103,     3,
          2170,  1012,   102,     0],
        [  101,  2129,   103,  1010,  2129,  9643,  2135,  3835,  1997,     2,
          2000,  2655,  1012,   102]], device='cuda:0')
labels: tensor([[-100, -100, -100, 1010, -100, 9643, 2135, -100, 2008, -100, -100, -100,
         -100, -100],
        [-100, -100, 3835, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 1.556706190109253


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.58it/s]
12/04/2023 14:51:53 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:53 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:53 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.38it/s]
12/04/2023 14:51:53 - INFO - __main__ -   epoch: 32
12/04/2023 14:51:53 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:53 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:53 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.12it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.11it/s]
12/04/2023 14:51:54 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-32
Epoch:  47%|âââââ     | 33/70 [01:10<01:17,  2.09s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:55 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:55 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:55 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835,  103,  103, 9643, 2135, 3835,  103,    2, 2000, 2655,
         1012,  102],
        [ 101,  103, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3,  103, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, 1010, 2129, -100, -100, -100, 1997, -100, -100, -100,
         -100, -100],
        [-100, 2129, -100, -100, -100, -100, -100, -100, -100, -100, 2170, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 3.471020460128784


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.66it/s]
12/04/2023 14:51:55 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:55 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:55 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.15it/s]
12/04/2023 14:51:55 - INFO - __main__ -   epoch: 33
12/04/2023 14:51:55 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:55 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:55 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.10it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.10it/s]
12/04/2023 14:51:56 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-33
Epoch:  49%|âââââ     | 34/70 [01:12<01:14,  2.06s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:57 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:57 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:57 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835, 15481,  2129,  9643,  2135,  3835,  2008,     3,
           103,  1012,   102,     0],
        [  101,  2129,  3835,  1010,  2129,  9643,  2135,  3835,  1997,     2,
           103,  2655,  1012,   102]], device='cuda:0')
labels: tensor([[-100, -100, -100, 1010, -100, -100, -100, -100, -100, -100, 2170, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2000, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 4.144077777862549


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.39it/s]
12/04/2023 14:51:57 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:57 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:57 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.76it/s]
12/04/2023 14:51:57 - INFO - __main__ -   epoch: 34
12/04/2023 14:51:57 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:57 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:57 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.06it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.06it/s]
12/04/2023 14:51:58 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-34
Epoch:  50%|âââââ     | 35/70 [01:14<01:11,  2.06s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:51:59 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:51:59 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:59 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835,  1010,  2129,   103,  2135,  3835,  1997,     2,
         13212,   103,  1012,   102],
        [  101,  2129,  3835,  1010,   103,  9643,  2135,  3835,  2008, 10213,
          2170,  1012,   102,     0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, 9643, -100, -100, -100, -100, 2000, 2655,
         -100, -100],
        [-100, -100, -100, -100, 2129, -100, -100, -100, -100,    3, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 6.58419942855835


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.53it/s]
12/04/2023 14:51:59 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:51:59 - INFO - __main__ -     Num examples = 2
12/04/2023 14:51:59 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.29it/s]
12/04/2023 14:51:59 - INFO - __main__ -   epoch: 35
12/04/2023 14:51:59 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:51:59 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:51:59 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.13it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.13it/s]
12/04/2023 14:52:00 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-35
Epoch:  51%|ââââââ    | 36/70 [01:16<01:09,  2.05s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:01 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:01 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:01 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,   103,  1010,  2129,  9643,   103,  3835,   103,   103,
           103,  1012,   102,     0],
        [  101,  2129,  3835, 10517,   103,  9643,   103,  3835,   103,   103,
          2000,  2655,  1012,   102]], device='cuda:0')
labels: tensor([[-100, -100, 3835, -100, -100, -100, 2135, -100, 2008,    3, 2170, -100,
         -100, -100],
        [-100, -100, -100, 1010, 2129, -100, 2135, -100, 1997,    2, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 6.103000640869141


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.78it/s]
12/04/2023 14:52:01 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:01 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:01 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.30it/s]
12/04/2023 14:52:01 - INFO - __main__ -   epoch: 36
12/04/2023 14:52:01 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:01 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:01 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.03it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.03it/s]
12/04/2023 14:52:02 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-36
Epoch:  53%|ââââââ    | 37/70 [01:18<01:08,  2.07s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:03 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:03 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:03 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835,   103,  2129,  9643,  2135,  3835,  2008, 21642,
          2170,  1012,   102,     0],
        [  101,   103,   103,  1010,  2129,  9643,  2135,  3835,  1997,   103,
          2000,  2655,  1012,   102]], device='cuda:0')
labels: tensor([[-100, -100, -100, 1010, -100, -100, -100, -100, -100,    3, -100, -100,
         -100, -100],
        [-100, 2129, 3835, -100, -100, -100, -100, -100, -100,    2, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 8.667645454406738


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.97it/s]
12/04/2023 14:52:03 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:03 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:03 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.28it/s]
12/04/2023 14:52:03 - INFO - __main__ -   epoch: 37
12/04/2023 14:52:03 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:03 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:03 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.15it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.15it/s]
12/04/2023 14:52:05 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-37
Epoch:  54%|ââââââ    | 38/70 [01:20<01:06,  2.07s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:05 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:05 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:05 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,    2, 2000, 2655,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129,  103, 2135, 3835, 2008,    3, 2170,  103,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, 9643, -100, -100, -100, -100, -100, 1012,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 0.7841842770576477


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.72it/s]
12/04/2023 14:52:05 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:05 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:05 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.01it/s]
12/04/2023 14:52:05 - INFO - __main__ -   epoch: 38
12/04/2023 14:52:05 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:05 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:05 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.07it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.07it/s]
12/04/2023 14:52:07 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-38
Epoch:  56%|ââââââ    | 39/70 [01:22<01:03,  2.04s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:07 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:07 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:07 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835,  103,    2, 2000, 2655,
         1012,  102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, 1997, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 0.09847582876682281


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.93it/s]
12/04/2023 14:52:07 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:07 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:07 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.50it/s]
12/04/2023 14:52:07 - INFO - __main__ -   epoch: 39
12/04/2023 14:52:07 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:07 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:07 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.95it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.94it/s]
12/04/2023 14:52:09 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-39
Epoch:  57%|ââââââ    | 40/70 [01:24<01:02,  2.10s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:09 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:09 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:09 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,  103, 2170,  103,
          102,    0],
        [ 101, 2129, 3835, 1010,  103, 9643, 2135, 3835, 1997,    2, 2000, 2655,
         1012,  102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100,    3, -100, 1012,
         -100, -100],
        [-100, -100, -100, -100, 2129, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 5.246656894683838


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.75it/s]
12/04/2023 14:52:09 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:09 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:09 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.10it/s]
12/04/2023 14:52:09 - INFO - __main__ -   epoch: 40
12/04/2023 14:52:09 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:09 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:09 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.97it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.96it/s]
12/04/2023 14:52:11 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-40
Epoch:  59%|ââââââ    | 41/70 [01:27<01:01,  2.10s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:11 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:11 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:11 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835,   103,  2129,  9643,  2135,  3835,   103, 26617,
          2170,   103,   102,     0],
        [  101,  2129,   103,  1010,  2129,  9643,  2135,  3835,  1997,     2,
          2000,  2655,   103,   102]], device='cuda:0')
labels: tensor([[-100, -100, -100, 1010, -100, -100, -100, -100, 2008,    3, -100, 1012,
         -100, -100],
        [-100, -100, 3835, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         1012, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 3.9268624782562256


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.80it/s]
12/04/2023 14:52:11 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:11 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:11 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.08it/s]
12/04/2023 14:52:11 - INFO - __main__ -   epoch: 41
12/04/2023 14:52:11 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:11 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:11 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.22it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.22it/s]
12/04/2023 14:52:13 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-41
Epoch:  60%|ââââââ    | 42/70 [01:28<00:57,  2.05s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:13 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:13 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:13 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,    2,  103, 2655,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2000, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1012,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 0.026361767202615738


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.39it/s]
12/04/2023 14:52:13 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:13 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:13 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.81it/s]
12/04/2023 14:52:13 - INFO - __main__ -   epoch: 42
12/04/2023 14:52:13 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:13 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:13 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.15it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.15it/s]
12/04/2023 14:52:15 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-42
Epoch:  61%|âââââââ   | 43/70 [01:30<00:55,  2.04s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:15 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:15 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:15 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101,  103, 3835, 1010, 2129, 9643, 2135, 3835,  103,    3,  103, 1012,
          102,    0],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,    2, 2000, 2655,
         1012,  102]], device='cuda:0')
labels: tensor([[-100, 2129, -100, -100, -100, -100, -100, -100, 2008, -100, 2170, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 4.041310787200928


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 32.32it/s]
12/04/2023 14:52:15 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:15 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:15 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.02it/s]
12/04/2023 14:52:15 - INFO - __main__ -   epoch: 43
12/04/2023 14:52:15 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:15 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:15 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.96it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.95it/s]
12/04/2023 14:52:17 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-43
Epoch:  63%|âââââââ   | 44/70 [01:33<00:53,  2.06s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:17 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:17 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:17 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,    2, 2000, 2655,
          103,  102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, 1010, -100, -100, -100, -100, -100, -100, -100, -100,
         1012, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 0.012906990945339203


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 32.16it/s]
12/04/2023 14:52:17 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:17 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:17 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.47it/s]
12/04/2023 14:52:17 - INFO - __main__ -   epoch: 44
12/04/2023 14:52:17 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:17 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:17 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.95it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.95it/s]
12/04/2023 14:52:19 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-44
Epoch:  64%|âââââââ   | 45/70 [01:35<00:51,  2.06s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:19 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:19 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:19 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,  103, 2000, 2655,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100,    2, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 7.541039943695068


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.44it/s]
12/04/2023 14:52:19 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:19 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:19 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.24it/s]
12/04/2023 14:52:19 - INFO - __main__ -   epoch: 45
12/04/2023 14:52:19 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:19 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:19 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.14it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.14it/s]
12/04/2023 14:52:21 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-45
Epoch:  66%|âââââââ   | 46/70 [01:37<00:48,  2.02s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:21 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:21 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:21 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,    2,  103, 2655,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, 1997, -100, 2000, -100,
         -100, -100],
        [-100, 2129, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1012,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 0.004815889522433281


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.93it/s]
12/04/2023 14:52:21 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:21 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:21 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.72it/s]
12/04/2023 14:52:21 - INFO - __main__ -   epoch: 46
12/04/2023 14:52:21 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:21 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:21 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.05it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.05it/s]
12/04/2023 14:52:23 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-46
Epoch:  67%|âââââââ   | 47/70 [01:39<00:46,  2.03s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:23 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:23 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:23 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010,  103, 9643, 2135, 3835, 2008,  103, 2170, 1012,
          102,    0],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135,  103, 1997,  103, 2000, 2655,
         1012,  102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, 2129, -100, -100, -100, -100,    3, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, 3835, -100,    2, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 6.453409194946289


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 32.09it/s]
12/04/2023 14:52:23 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:23 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:23 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 30.32it/s]
12/04/2023 14:52:23 - INFO - __main__ -   epoch: 47
12/04/2023 14:52:23 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:23 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:23 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.96it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.96it/s]
12/04/2023 14:52:25 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-47
Epoch:  69%|âââââââ   | 48/70 [01:41<00:44,  2.03s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:25 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:25 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:25 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135,  103, 1997,    2, 2000, 2655,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, 3835, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, 3835, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 1.193041205406189


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.48it/s]
12/04/2023 14:52:25 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:25 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:25 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.09it/s]
12/04/2023 14:52:25 - INFO - __main__ -   epoch: 48
12/04/2023 14:52:25 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:25 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:25 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.06it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.06it/s]
12/04/2023 14:52:27 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-48
Epoch:  70%|âââââââ   | 49/70 [01:43<00:41,  2.00s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:27 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:27 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:27 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135,  103, 1997,    2, 2000,  103,
         1012,  102],
        [ 101, 2129, 3835,  103, 2129,  103, 2135, 3835, 2008,    3, 2170,  103,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, 3835, -100, -100, -100, 2655,
         -100, -100],
        [-100, -100, -100, 1010, -100, 9643, -100, -100, -100, -100, -100, 1012,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 1.8610894680023193


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.73it/s]
12/04/2023 14:52:27 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:27 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:27 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.15it/s]
12/04/2023 14:52:27 - INFO - __main__ -   epoch: 49
12/04/2023 14:52:27 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:27 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:27 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.05it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.05it/s]
12/04/2023 14:52:29 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-49
Epoch:  71%|ââââââââ  | 50/70 [01:45<00:40,  2.02s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:29 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:29 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:29 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835,  1010,  2129,  9643,  2135,  3835,  1997,     2,
          2000,  2655,  1012,   102],
        [  101,  2129,  3835,  1010,  2129,  9643,  2135,  3835,  2008,     3,
         11217,  1012,   102,     0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, -100,    3, 2170, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 11.316303253173828


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.04it/s]
12/04/2023 14:52:29 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:29 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:29 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.24it/s]
12/04/2023 14:52:29 - INFO - __main__ -   epoch: 50
12/04/2023 14:52:29 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:29 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:29 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.01it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.01it/s]
12/04/2023 14:52:31 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-50
Epoch:  73%|ââââââââ  | 51/70 [01:47<00:38,  2.01s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:31 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:31 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:31 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101,  103, 3835, 1010, 2129, 9643,  103, 3835, 1997,    2,  103, 2655,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129, 9643,  103, 3835, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, 2129, -100, -100, -100, -100, 2135, -100, -100, -100, 2000, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, 2135, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 0.020027516409754753


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.65it/s]
12/04/2023 14:52:31 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:31 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:31 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.45it/s]
12/04/2023 14:52:31 - INFO - __main__ -   epoch: 51
12/04/2023 14:52:31 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:31 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:31 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.22it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.22it/s]
12/04/2023 14:52:33 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-51
Epoch:  74%|ââââââââ  | 52/70 [01:49<00:35,  1.97s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:33 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:33 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:33 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835,  103,  103, 2000,  103,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 6726, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, 1997,    2, -100, 2655,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, 3835, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 7.1479034423828125


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.25it/s]
12/04/2023 14:52:33 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:33 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:33 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.61it/s]
12/04/2023 14:52:33 - INFO - __main__ -   epoch: 52
12/04/2023 14:52:33 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:33 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:33 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.27it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.27it/s]
12/04/2023 14:52:35 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-52
Epoch:  76%|ââââââââ  | 53/70 [01:51<00:33,  1.98s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:35 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:35 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:35 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,    2, 2000, 2655,
         1012,  102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = nan


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.12it/s]
12/04/2023 14:52:35 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:35 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:35 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.24it/s]
12/04/2023 14:52:35 - INFO - __main__ -   epoch: 53
12/04/2023 14:52:35 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:35 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:35 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.02it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.02it/s]
12/04/2023 14:52:37 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-53
Epoch:  77%|ââââââââ  | 54/70 [01:53<00:32,  2.00s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:37 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:37 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:37 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135,  103, 2008,    3, 2170, 1012,
          102,    0],
        [ 101, 2129, 3835, 1010, 2129, 9643,  103, 3835, 1997,  103, 2000, 2655,
         1012,  102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, 3835, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, 2135, -100, -100,    2, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 2.3451499938964844


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.29it/s]
12/04/2023 14:52:37 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:37 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:37 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.50it/s]
12/04/2023 14:52:37 - INFO - __main__ -   epoch: 54
12/04/2023 14:52:37 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:37 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:37 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.01it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.01it/s]
12/04/2023 14:52:39 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-54
Epoch:  79%|ââââââââ  | 55/70 [01:55<00:30,  2.02s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:39 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:39 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:39 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835,  1010,  2129,  9643,   103,  3835,  1997,     2,
         30138,  2655,  1012,   102],
        [  101,  2129,  3835,  1010,  2129,  9643,  2135,  3835,   103,     3,
          2170,  1012,   102,     0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, 2135, -100, -100, -100, 2000, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, 2008, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 0.7813987135887146


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.35it/s]
12/04/2023 14:52:39 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:39 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:39 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.56it/s]
12/04/2023 14:52:39 - INFO - __main__ -   epoch: 55
12/04/2023 14:52:39 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:39 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:39 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.20it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.20it/s]
12/04/2023 14:52:41 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-55
Epoch:  80%|ââââââââ  | 56/70 [01:57<00:28,  2.01s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:41 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:41 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:41 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835,  103,    2, 2000, 2655,
         1012,  102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, 1997, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 0.1463252454996109


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.44it/s]
12/04/2023 14:52:41 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:41 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:41 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.30it/s]
12/04/2023 14:52:41 - INFO - __main__ -   epoch: 56
12/04/2023 14:52:41 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:41 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:41 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.13it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.13it/s]
12/04/2023 14:52:43 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-56
Epoch:  81%|âââââââââ | 57/70 [01:59<00:26,  2.01s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:43 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:43 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:43 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010,  103, 9643,  103, 3835,  103,    2, 2000, 2655,
          103,  102],
        [ 101, 2129,  103, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, 2129, -100, 2135, -100, 1997, -100, -100, -100,
         1012, -100],
        [-100, -100, 3835, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 0.5249672532081604


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.95it/s]
12/04/2023 14:52:43 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:43 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:43 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.55it/s]
12/04/2023 14:52:43 - INFO - __main__ -   epoch: 57
12/04/2023 14:52:43 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:43 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:43 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.05it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.04it/s]
12/04/2023 14:52:45 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-57
Epoch:  83%|âââââââââ | 58/70 [02:01<00:24,  2.02s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:45 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:45 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:45 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835,  1010,  2129,  9643,  2135,  3835,  1997, 20667,
           103,  2655,  1012,   102],
        [  101,  2129,  3835,  1010,  2129,  9643,  2135,  3835,   103,     3,
          2170,   103,   102,     0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100,    2, 2000, -100,
         -100, -100],
        [-100, -100, -100, 1010, -100, -100, -100, 3835, 2008, -100, -100, 1012,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 2.6981847286224365


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.22it/s]
12/04/2023 14:52:45 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:45 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:45 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.79it/s]
12/04/2023 14:52:46 - INFO - __main__ -   epoch: 58
12/04/2023 14:52:46 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:46 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:46 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.00it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.99it/s]
12/04/2023 14:52:47 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-58
Epoch:  84%|âââââââââ | 59/70 [02:03<00:22,  2.01s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:47 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:47 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:47 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129,  103, 1010, 2129, 9643, 2135, 3835, 2008,    3,  103, 1012,
          102,    0],
        [ 101,  103, 3835, 1010,  103, 9643, 2135, 3835, 1997,    2,  103, 2655,
         1012,  102]], device='cuda:0')
labels: tensor([[-100, -100, 3835, -100, -100, -100, -100, -100, -100, -100, 2170, -100,
         -100, -100],
        [-100, 2129, -100, -100, 2129, -100, -100, -100, -100, -100, 2000, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 2.8049702644348145


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.41it/s]
12/04/2023 14:52:47 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:47 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:47 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.86it/s]
12/04/2023 14:52:48 - INFO - __main__ -   epoch: 59
12/04/2023 14:52:48 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:48 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:48 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.02it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.02it/s]
12/04/2023 14:52:49 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-59
Epoch:  86%|âââââââââ | 60/70 [02:05<00:19,  2.00s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:49 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:49 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:49 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835,   103,  2129,   103,  2135,   103,  2008,     3,
         17947,   103,   102,     0],
        [  101,  2129,  3835,  1010,  2129,  9643,  2135,  3835,  1997, 16288,
          2000,  2655,  1012,   102]], device='cuda:0')
labels: tensor([[-100, -100, -100, 1010, -100, 9643, -100, 3835, -100, -100, 2170, 1012,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, -100,    2, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 4.836207866668701


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.23it/s]
12/04/2023 14:52:49 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:49 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:49 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.26it/s]
12/04/2023 14:52:49 - INFO - __main__ -   epoch: 60
12/04/2023 14:52:49 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:49 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:49 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.00it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.00it/s]
12/04/2023 14:52:51 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-60
Epoch:  87%|âââââââââ | 61/70 [02:07<00:17,  2.00s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:51 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:51 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:51 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129,  103, 2135, 3835, 2008,    3,  103, 1012,
          102,    0],
        [ 101,  103, 3835, 1010, 2129, 9643, 2135,  103, 1997,  103, 2000,  103,
         1012,  102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, 9643, -100, -100, -100, -100, 2170, -100,
         -100, -100],
        [-100, 2129, -100, -100, -100, -100, -100, 3835, -100,    2, -100, 2655,
         1012, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 3.9600493907928467


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.84it/s]
12/04/2023 14:52:51 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:51 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:51 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.42it/s]
12/04/2023 14:52:51 - INFO - __main__ -   epoch: 61
12/04/2023 14:52:51 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:51 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:51 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.98it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.98it/s]
12/04/2023 14:52:53 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-61
Epoch:  89%|âââââââââ | 62/70 [02:09<00:16,  2.01s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:53 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:53 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:53 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101,  103, 3835, 1010, 2129, 9643, 2135, 3835, 1997,  103, 2000, 2655,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835,  103,    3,  103, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, 2129, -100, -100, -100, -100, -100, -100, 1997,    2, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, 2008, -100, 2170, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 2.2678208351135254


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.13it/s]
12/04/2023 14:52:53 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:53 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:53 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.98it/s]
12/04/2023 14:52:53 - INFO - __main__ -   epoch: 62
12/04/2023 14:52:53 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:53 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:53 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.12it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.12it/s]
12/04/2023 14:52:55 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-62
Epoch:  90%|âââââââââ | 63/70 [02:11<00:13,  2.00s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:55 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:55 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:55 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,  3835,  1010,  2129,  9643,  2135,  3835,  1997,     2,
          2000,  2655,   103,   102],
        [  101,  2129,  3835,  1010,  2129,  9643, 19433,  3835,  2008,     3,
          2170,  1012,   102,     0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         1012, -100],
        [-100, -100, -100, -100, -100, -100, 2135, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 1.5144398212432861


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.80it/s]
12/04/2023 14:52:55 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:55 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:55 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.44it/s]
12/04/2023 14:52:55 - INFO - __main__ -   epoch: 63
12/04/2023 14:52:55 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:55 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:55 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.10it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.10it/s]
12/04/2023 14:52:57 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-63
Epoch:  91%|ââââââââââ| 64/70 [02:13<00:11,  2.00s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:57 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:57 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:57 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3,  103, 1012,
          102,    0],
        [ 101, 2129, 3835, 1010,  103, 9643, 2135, 8545, 1997,  103, 2000, 2655,
         1012,  102]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2170, -100,
         -100, -100],
        [-100, -100, -100, -100, 2129, -100, 2135, 3835, -100,    2, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 4.831064701080322


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.76it/s]
12/04/2023 14:52:57 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:57 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:57 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.49it/s]
12/04/2023 14:52:57 - INFO - __main__ -   epoch: 64
12/04/2023 14:52:57 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:57 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:57 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.15it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.15it/s]
12/04/2023 14:52:59 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-64
Epoch:  93%|ââââââââââ| 65/70 [02:15<00:10,  2.01s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:52:59 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:52:59 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:59 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 1997,    2,  103, 2655,
         1012,  102],
        [ 101, 2129,  103, 1010,  103, 9643, 2135, 3835, 2008,    3, 2170,  103,
          102,    0]], device='cuda:0')
labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100,    2, 2000, -100,
         -100, -100],
        [-100, -100, 3835, -100, 2129, -100, -100, 3835, -100, -100, -100, 1012,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 0.8538663983345032


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.23it/s]
12/04/2023 14:52:59 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:52:59 - INFO - __main__ -     Num examples = 2
12/04/2023 14:52:59 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.08it/s]
12/04/2023 14:52:59 - INFO - __main__ -   epoch: 65
12/04/2023 14:52:59 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:52:59 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:52:59 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.14it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.14it/s]
12/04/2023 14:53:01 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-65
Epoch:  94%|ââââââââââ| 66/70 [02:17<00:08,  2.02s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:53:01 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:53:01 - INFO - __main__ -     Num examples = 2
12/04/2023 14:53:01 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101,  103, 3835,  103, 2129, 9643, 2135, 3835,  103,    2, 2000, 2655,
         1012,  102],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135, 3835, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, 2129, -100, 1010, -100, -100, -100, -100, 1997,    2, -100, -100,
         -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 1.5701048374176025


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.46it/s]
12/04/2023 14:53:01 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:53:01 - INFO - __main__ -     Num examples = 2
12/04/2023 14:53:01 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.09it/s]
12/04/2023 14:53:02 - INFO - __main__ -   epoch: 66
12/04/2023 14:53:02 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:53:02 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:53:02 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.15it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.14it/s]
12/04/2023 14:53:03 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-66
Epoch:  96%|ââââââââââ| 67/70 [02:19<00:06,  2.02s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:53:04 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:53:04 - INFO - __main__ -     Num examples = 2
12/04/2023 14:53:04 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[ 101,  103, 3835, 1010, 2129, 9643, 2135, 3835, 1997,    2, 2000, 2655,
          103,  102],
        [ 101, 2129, 3835, 1010, 2129, 9643, 2135,  103, 2008,    3, 2170, 1012,
          102,    0]], device='cuda:0')
labels: tensor([[-100, 2129, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         1012, -100],
        [-100, -100, -100, -100, 2129, -100, -100, 3835, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 0.3409772217273712


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.53it/s]
12/04/2023 14:53:04 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:53:04 - INFO - __main__ -     Num examples = 2
12/04/2023 14:53:04 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 32.64it/s]
12/04/2023 14:53:04 - INFO - __main__ -   epoch: 67
12/04/2023 14:53:04 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:53:04 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:53:04 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.03it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.03it/s]
12/04/2023 14:53:05 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-67
Epoch:  97%|ââââââââââ| 68/70 [02:21<00:04,  2.02s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:53:06 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:53:06 - INFO - __main__ -     Num examples = 2
12/04/2023 14:53:06 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101,  2129,   103,  1010,   103,  9643, 14556,   103,  1997,     2,
          2000,  2655,  1012,   102],
        [  101,  2129,  3835,   103,  2129,  9643,  2135,  3835,  2008,   103,
          2170,  1012,   102,     0]], device='cuda:0')
labels: tensor([[-100, -100, 3835, -100, 2129, -100, 2135, 3835, -100, -100, -100, -100,
         -100, -100],
        [-100, -100, 3835, 1010, -100, -100, 2135, -100, -100,    3, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0')
loss = 4.395876407623291


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.77it/s]
12/04/2023 14:53:06 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:53:06 - INFO - __main__ -     Num examples = 2
12/04/2023 14:53:06 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.02it/s]
12/04/2023 14:53:06 - INFO - __main__ -   epoch: 68
12/04/2023 14:53:06 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:53:06 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:53:06 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.00it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.00it/s]
12/04/2023 14:53:07 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-68
Epoch:  99%|ââââââââââ| 69/70 [02:23<00:02,  2.03s/it]
Iteration:   0%|          | 0/1 [00:00<?, ?it/s][A12/04/2023 14:53:08 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:53:08 - INFO - __main__ -     Num examples = 2
12/04/2023 14:53:08 - INFO - __main__ -     Batch size = 16
special_tokens_mask:
tensor([[ True, False, False, False, False, False, False, False, False, False,
         False, False,  True,  True],
        [ True, False, False, False, False, False, False, False, False, False,
         False, False, False,  True]])
nonce_tokens_mask:
tensor([[False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False],
        [False, False, False, False, False, False, False, False, False,  True,
         False, False, False, False]])
inputs: tensor([[  101, 23947,  3835,  1010,  2129,  9643,  2135,   103,  2008,   103,
          2170,  1012,   102,     0],
        [  101,  2129,  3835,  1651,  2129,  9643,  2135,  3835,  1997,     2,
          2000,  2655,  1012,   102]], device='cuda:0')
labels: tensor([[-100, 2129, -100, -100, -100, -100, -100, 3835, -100,    3, -100, -100,
         -100, -100],
        [-100, -100, -100, 1010, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100]], device='cuda:0')
attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')
loss = 3.7264227867126465


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.82it/s]
12/04/2023 14:53:08 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:53:08 - INFO - __main__ -     Num examples = 2
12/04/2023 14:53:08 - INFO - __main__ -     Batch size = 16


Evaluating:   0%|          | 0/1 [00:00<?, ?it/s][A[AEvaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.47it/s]
12/04/2023 14:53:08 - INFO - __main__ -   epoch: 69
12/04/2023 14:53:08 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:53:08 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:53:08 - INFO - __main__ -   ctg1_diff: 0.0

Iteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.03it/s][AIteration: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.02it/s]
12/04/2023 14:53:09 - INFO - __main__ -   Saving model checkpoint to checkpoints/bert-base-uncased/unused_pairs_1/testexp_nv_unused_token_numbers_1_2_learning_rate_0.001_seed_1/checkpoint-69
Epoch: 100%|ââââââââââ| 70/70 [02:25<00:00,  2.03s/it]Epoch: 100%|ââââââââââ| 70/70 [02:25<00:00,  2.08s/it]
12/04/2023 14:53:09 - INFO - __main__ -    global_step = 70, average loss = 3.3567282262524323
12/04/2023 14:53:09 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:53:09 - INFO - __main__ -     Num examples = 2
12/04/2023 14:53:09 - INFO - __main__ -     Batch size = 16
Best dev for structurally different:  0.5 [-1, 0, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]
Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]Evaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.60it/s]
12/04/2023 14:53:09 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:53:09 - INFO - __main__ -     Num examples = 2
12/04/2023 14:53:09 - INFO - __main__ -     Batch size = 16
Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]Evaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.39it/s]
12/04/2023 14:53:09 - INFO - __main__ -   epoch: 0
12/04/2023 14:53:09 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:53:09 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:53:09 - INFO - __main__ -   ctg1_diff: 0.0
12/04/2023 14:53:09 - INFO - __main__ -   ***** Running evaluation on ctg0_diff *****
12/04/2023 14:53:09 - INFO - __main__ -     Num examples = 2
12/04/2023 14:53:09 - INFO - __main__ -     Batch size = 16
Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]Evaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.10it/s]
12/04/2023 14:53:09 - INFO - __main__ -   ***** Running evaluation on ctg1_diff *****
12/04/2023 14:53:09 - INFO - __main__ -     Num examples = 2
12/04/2023 14:53:09 - INFO - __main__ -     Batch size = 16
Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]Evaluating: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.96it/s]
12/04/2023 14:53:09 - INFO - __main__ -   epoch: 69
12/04/2023 14:53:09 - INFO - __main__ -   all_diff: 0.5
12/04/2023 14:53:09 - INFO - __main__ -   ctg0_diff: 1.0
12/04/2023 14:53:09 - INFO - __main__ -   ctg1_diff: 0.0
